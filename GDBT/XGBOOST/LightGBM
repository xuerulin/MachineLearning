集成学习
boosting
串行的方式训练基分类器，各分类器之间有依赖。每次训练时，对前一层基分类器分错的样本给与更高的权重
bagging
bagging是Bootstrap aggregating的意思，各分类器之间无强依赖，可以并行，同时根据权重得到最后结果。
方差&偏差

偏差:
【背下来】偏差是指由有所采样得到的大小为m的训练数据集，训练出的所有模型的输出的平均值和真实模型输出之间的偏差。
通常是由对学习算法做了错误的假设导致的
描述模型输出结果的期望与样本真实结果的差距。分类器表达能力有限导致的系统性错误，表现在训练误差不收敛
方差:
【背下来】是指有所有采样得到的大小为m的训练数据集，训练出的所有模型的输出的方差
描述模型对于给定值的输出稳定性。分类器对样本分布过于敏感，到指在训练样本较少的时候，出现过拟合

基分类器的错误，是偏差和方差之和
boosting方法通过逐步聚焦分类器分错的样本，减少集成分类器的偏差
Bagging采用分而治之的策略，通过对样本多次采样，分别训练多个模型，减少方差

为什么决策树是常用的基分类器
可以方便地将样本权重整合到训练过程中，不需要使用过采样的方法来调整样本权重
决策树的表达能力和繁华能力，可以通过调节树的层数来做折中
数据样本扰动对决策树影响较大，因此不同子样本集生成的基分类器随机性就较大。这样的不稳定学习器更适合作为基分类器。
插一句，神经网络也适合做基分类器

Adaboost(Adaptive Boosting)
Boosting分类方法，其过程如下所示：
1.先通过对N个训练数据的学习得到第一个弱分类器h1；
2.将h1分错的数据和其他的新数据一起构成一个新的有N个训练数据的样本，通过对这个样本的学习得到第二个弱分类器h2；
3.将h1和h2都分错了的数据加上其他的新数据构成另一个新的有N个训练数据的样本，通过对这个样本的学习得到第三个弱分类器h3；
4.最终经过提升的强分类器h_final=Majority Vote(h1,h2,h3)。即某个数据被分为哪一类要通过h1,h2,h3的多数表决。
上述Boosting算法，存在两个问题：
1.如何调整训练集，使得在训练集上训练弱分类器得以进行。
2.如何将训练得到的各个弱分类器联合起来形成强分类器。
针对以上两个问题，AdaBoost算法进行了调整：
1.使用加权后选取的训练数据代替随机选取的训练数据，这样将训练的焦点集中在比较难分的训练数据上。
2.将弱分类器联合起来时，使用加权的投票机制代替平均投票机制。让分类效果好的弱分类器具有较大的权重，而分类效果差的分类器具有较小的权重。
Python代码：
from sklearn.ensemble import AdaBoostClassifier # 分类任务
from sklearn.ensemble import AdaBoostRegressor  # 回归任务
from skleran.tree import DecisionTreeClassifier
base_estimator = DecisionTreeClassifier()
clf = AdaBoostClassifier(n_estimators=100, base_estimator=base_estimator, learning_rate=1)
# 使用决策树作为基分类器
clf.fit(x_train, y_train)
参数：
base_estimator:基分类器，默认是决策树，在该分类器基础上进行boosting，理论上可以是任意一个分类器，但是如果是其他分类器时需要指明样本权重。
n_estimators:基分类器提升（循环）次数，默认是50次，这个值过大，模型容易过拟合；值过小，模型容易欠拟合。
learning_rate:学习率，表示梯度收敛速度，默认为1，如果过大，容易错过最优值，如果过小，则收敛速度会很慢；该值需要和n_estimators进行一个权衡，当分类器迭代次数较少时，学习率可以小一些，当迭代次数较多时，学习率可以适当放大。
algorithm:boosting算法，也就是模型提升准则，有两种方式SAMME, 和SAMME.R两种，默认是SAMME.R，两者的区别主要是弱学习器权重的度量，前者是对样本集预测错误的概率进行划分的，后者是对样本集的预测错误的比例，即错分率进行划分的，默认是用的SAMME.R
random_state:随机种子设置。

GBDT(Gradient Boosting Decision Tree)
众所周知，弱学习器的预测效果是有限的，也就是说其预测结果与真实结果存在一定的差距（即残差），那么我们可以训练第二个弱学习器，将残差作为目标来学习，
然后把两个弱学习器的预测结果相加，这是仍旧存在一定的残差，就可以继续上一过程，继续学习弱学习器，直至达到目标。这是一种提升（Boosting）的思想。
这就是GBM的核心思想。而GBDT 可以说是对 GBM 的一种实现，其弱学习器使用的是Regression Tree。
GBDT的一般步骤：
Step 1: 初始化。初始化y_hat在第0时刻的值。
Step 2：求残差。通过类似梯度下降法的思路，每次y都向梯度下降的方向挪一小步。只是在GBDT，y挪的一小步并不是一个variable，而是一个function。
Step 3：构建决策树。使用决策树逼近这个残差 –g，得到第t个决策树：f_t。
Step 4：求叶节点权重。
Step 5：更新输出y。 y(t)=y(t-1)+learningrate*ft
Python代码:
from sklearn.ensemble import GradientBoostingClassifier  # 分类任务
from sklearn.ensemble import GradientBoostingRegressor   # 回归任务
clf = GradientBoostingClassfier(n_estimators=100, learning_rate=1.0, max_depth=1)
clf.fit(X_train, y_train)
参数:
loss:字符串，指定损失函数，可以为：
--'deviance'：此时损失函数为对数损失函数
--'exponential'：此时使用指数损失函数，注意该损失只能用于二分类。
n_estimators :整数，指定基础决策树的数量（默认为100）。GBDT对过拟合有很好的鲁棒性，因此该值越大越好。
learning_rate:浮点数。默认为1.它用于每一步的步长，防止步长太大而跨过了极值点。通常learning_rate越小，则需要的基础分类器数量就会越多，因此在learning_rate和n_estimators之间会有所折中。
max_depth:整数或者None，指定每一个基础决策树模型的最大深度。如果max_leaf_noeds不是None，则忽略此参数。
criterion:字符串。用来衡量切分的质量。默认为'friedman_mse'。可以为
--'friedman_mse'： 改进型的均方误差
--'mse'：标准的均方误差
--'mae'：平均绝对误差。
min_samples_split:整数，指定了每个基础决策树模型分裂所需最小样本数。
min_samples_leaf:整数，指定了每个基础决策树模型叶节点所包含的最小样本数。
min_weight_fraction_leaf:一个浮点数。叶节点的最小加权权重。当不提供sample_weight时，样本的权重是相等的。
subsample:一个浮点数，指定了提取原始训练集中的一个子集用于训练基础决策树，该参数就是子集占原始训练集的大小，0<subsample<10<subsample<1。
max_features:一个整数，浮点数或者None。代表节点分裂是参与判断的最大特征数。整数为个数，浮点数为所占比重。
max_leaf_nodes:为整数或者None，指定了每个基础决策树模型的最大叶节点数量。
min_impurity_split:一个浮点数，指定在树生长的过程中，节点分裂的阈值，默认为1e-7。
init:一个基础分类器对象或者None，该分类器对象用于执行初始的预测。如果为None，则使用loss.init_estimator。
verbose:一个整数，如果为0则不输出日志，如果为1，则每隔一段时间输出日志，大于1输出日志会更频繁。
warm_start:布尔值。当为True是，则继续使用上一次训练结果。否则重新开始训练。
random_state:一个整数或者一个RandomState实例，或者None。
             如果为整数，指定随机数生成器的种子。
             如果为RandomState，指定随机数生成器。
             如果为None，指定使用默认的随机数生成器。
presort:一个布尔值或者‘auto’，是否对数据进行显示以加速最佳分割的发现。默认情况下，自动模式将在密集的数据上使用presorting，默认为稀疏数据的正常排序。
在稀疏的数据上设置显示值将会导致错误。
Gradient Boosting是非常经典而又重要的提升方法，其与AdaBoost一样都是将弱分类器组合成为一个强分类器，但是其大致区别有:
1.Gradient Boosting通过残差来变量的改变错误分类的权重，而AdaBoost是直接修改分类错误样本的权重
2.Gradient Boosting的基分类器较多使用完整的决策树，而AdaBoost一般使用二层决策树

XGBoost(Extreme Gradient Boosting)
XGBoost是GBDT的一种高效实现

变化1：提高了精度 – 对Loss的近似从一阶到二阶。。传统GBDT只使用了一阶导数对loss进行近似，而XGBoost对Loss进行泰勒展开，取一阶导数和二阶导数。
      同时，XGBoost的Loss考虑了正则化项，包含了对复杂模型的惩罚，比如叶节点的个数、树的深度等等。通过对Loss的推导，得到了构建树时不同树的score。
变化2：提高了效率 – 近似算法加快树的构建。XGBoost支持几种构建树的方法。
      第一：使用贪心算法，分层添加decision tree的叶节点。对每个叶节点，对每个feature的所有instance值进行排序，得到所有可能的split。
      选择score最大的split，作为当前节点。
      第二：使用quantile对每个feature的所有instance值进行分bin，将数据离散化。
      第三：使用histogram对每个feature的所有instance值进行分bin，将数据离散化。
变化3：提高了效率 – 并行化与cache access。XGBoost在系统上设计了一些方便并行计算的数据存储方法，同时也对cache access进行了优化。
      这些设计使XGBoost的运算表现在传统GBDT系统上得到了很大提升。
XGBoost优势：
.显式地将树模型的复杂度作为正则项加在优化目标
.公式推导里用到了二阶导数信息，而普通的GBDT只用到一阶
.允许使用列抽样(column(feature)sampling)来防止过拟合，借鉴了Random Forest的思想，sklearn里的gbm好像也有类似实现。
.实现了一种分裂节点寻找的近似算法，用于加速和减小内存消耗。
.节点分裂算法能自动利用特征的稀疏性。
.样本数据事先排好序并以block的形式存储，利于并行计算
.penalty function Omega主要是对树的叶子数和叶子分数做惩罚，这点确保了树的简单性。
.支持分布式计算可以运行在MPI，YARN上，得益于底层支持容错的分布式通信框架rabit。

XGBT的目标函数：我们知道模型的预测精度由模型的偏差和方差共同决定，损失函数代表了模型的偏差，想要方差小则需要简单的模型，
               所以目标函数由模型的损失函数 [L] 与抑制模型复杂度的正则项 [Ω] 组成。
Xgboost 支持两种分裂节点的方法——贪心算法和近似算法。
1）贪心算法
1.从深度为0的树开始，对每个叶节点枚举所有的可用特征；
2.针对每个特征，把属于该节点的训练样本根据该特征值进行升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的分裂收益；
3.选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，在该节点上分裂出左右两个新的叶节点，并为每个新节点关联对应的样本集
4.回到第 1 步，递归执行到满足特定条件为止
2）近似算法
贪婪算法可以的到最优解，但当数据量太大时则无法读入内存进行计算，近似算法主要针对贪婪算法这一缺点给出了近似最优解。
对于每个特征，只考察分位点可以减少计算复杂度。
该算法会首先根据特征分布的分位数提出候选划分点，然后将连续型特征映射到由这些候选点划分的桶中，然后聚合统计信息找到所有区间的最佳分裂点。
在提出候选切分点时有两种策略：
Global：学习每棵树前就提出候选切分点，并在每次分裂时都采用这种分割；
Local：每次分裂前将重新提出候选切分点。
稀疏感知算法
XGBoost 在构建树的节点过程中只考虑非缺失值的数据遍历，而为每个节点增加了一个缺省方向，当样本相应的特征值缺失时，可以被归类到缺省方向上，
最优的缺省方向可以从数据中学到。至于如何学到缺省值的分支，其实很简单，分别枚举特征缺省的样本归为左右分支后的增益，
选择增益最大的枚举项即为最优缺省方向。

LightGBM
从 LightGBM 名字我们可以看出其是轻量级（Light）的梯度提升机（GBM），其相对 XGBoost 具有训练速度快、内存占用低的特点。
对于XGBoost的缺点，LightGBM 为了解决这些问题提出了以下几点解决方案：
1.单边梯度抽样算法；
2.直方图算法；
3.互斥特征捆绑算法；
4.基于最大深度的 Leaf-wise 的垂直生长算法；
5.类别特征最优分割；
6.特征并行和数据并行；
7.缓存优化。
2.1.1 单边梯度抽样算法
GBDT 算法的梯度大小可以反应样本的权重，梯度越小说明模型拟合的越好，单边梯度抽样算法（Gradient-based One-Side Sampling, GOSS）
利用这一信息对样本进行抽样，减少了大量梯度小的样本，在接下来的计算锅中只需关注梯度高的样本，极大的减少了计算量。
GOSS 算法保留了梯度大的样本，并对梯度小的样本进行随机抽样，为了不改变样本的数据分布，在计算增益时为梯度小的样本引入一个常数进行平衡。
2.1.2 直方图算法
1）直方图算法
直方图算法的基本思想是将连续的特征离散化为 k 个离散特征，同时构造一个宽度为 k 的直方图用于统计信息（含有 k 个 bin）。
利用直方图算法我们无需遍历数据，只需要遍历 k 个 bin 即可找到最佳分裂点。
特征离散化的具有很多优点，如存储方便、运算更快、鲁棒性强、模型更加稳定等等。对于直方图算法来说最直接的有以下两个优点（以 k=256 为例）：
内存占用更小：XGBoost 需要用 32 位的浮点数去存储特征值，并用 32 位的整形去存储索引，而 LightGBM 只需要用 8 位去存储直方图，相当于减少了 1/8；
计算代价更小：计算特征分裂增益时，XGBoost 需要遍历一次数据找到最佳分裂点，而 LightGBM 只需要遍历一次 k 次，直接将时间复杂度降低。
虽然将特征离散化后无法找到精确的分割点，可能会对模型的精度产生一定的影响，但较粗的分割也起到了正则化的效果，一定程度上降低了模型的方差。
2) 直方图加速
在构建叶节点的直方图时，我们还可以通过父节点的直方图与相邻叶节点的直方图相减的方式构建，从而减少了一半的计算量。
在实际操作过程中，我们还可以先计算直方图小的叶子节点，然后利用直方图作差来获得直方图大的叶子节点。
2.1.3 互斥特征捆绑算法
高维特征往往是稀疏的，而且特征间可能是相互排斥的（如两个特征不同时取非零值），如果两个特征并不完全互斥（如只有一部分情况下是不同时取非零值），
可以用互斥率表示互斥程度。互斥特征捆绑算法（Exclusive Feature Bundling, EFB）指出如果将一些特征进行融合绑定，则可以降低特征数量。
针对这种想法，我们会遇到两个问题：

哪些特征可以一起绑定？
特征绑定后，特征值如何确定？
对于问题一：EFB 算法利用特征和特征间的关系构造一个加权无向图，并将其转换为图着色算法。
我们知道图着色是个 NP-Hard 问题，故采用贪婪算法得到近似解，具体步骤如下：
1.构造一个加权无向图，顶点是特征，边是两个特征间互斥程度；
2.根据节点的度进行降序排序，度越大，与其他特征的冲突越大；
3.遍历每个特征，将它分配给现有特征包，或者新建一个特征包，是的总体冲突最小。
对于问题二：论文给出特征合并算法，其关键在于原始特征能从合并的特征中分离出来。
假设 Bundle 中有两个特征值，A 取值为 [0, 10]、B 取值为 [0, 20]，为了保证特征 A、B 的互斥性，我们可以给特征 B 添加一个偏移量转换为 [10, 30]，
Bundle 后的特征其取值为 [0, 30]，这样便实现了特征合并。

2.1.4 带深度限制的 Leaf-wise 算法
在建树的过程中有两种策略：
Level-wise：基于层进行生长，直到达到停止条件；
Leaf-wise：每次分裂增益最大的叶子节点，直到达到停止条件。
XGBoost 采用 Level-wise 的增长策略，方便并行计算每一层的分裂节点，提高了训练速度，但同时也因为节点增益过小增加了很多不必要的分裂，
降低了计算量；LightGBM 采用 Leaf-wise 的增长策略减少了计算量，配合最大深度的限制防止过拟合，由于每次都需要计算增益最大的节点，所以无法并行分裂。
XGBoost采用的是按层生长level（depth）-wise生长策略，能够同时分裂同一层的叶子，从而进行多线程优化，不容易过拟合；但不加区分的对待同一层的叶子，
        带来了很多没必要的开销。因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。
LightGBM采用leaf-wise生长策略，每次从当前所有叶子中找到分裂增益最大（一般也是数据量最大）的一个叶子，然后分裂，如此循环。因此同Level-wise相比，
        在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。
         因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。
2.1.5 类别特征最优分割

大部分的机器学习算法都不能直接支持类别特征，一般都会对类别特征进行编码，然后再输入到模型中。常见的处理类别特征的方法为 one-hot 编码，
但我们知道对于决策树来说并不推荐使用 one-hot 编码：
1.会产生样本切分不平衡问题，切分增益会非常小。如，国籍切分后，会产生是否中国，是否美国等一系列特征，这一系列特征上只有少量样本为 1，大量样本为 0。
这种划分的增益非常小：较小的那个拆分样本集，它占总样本的比例太小。无论增益多大，乘以该比例之后几乎可以忽略；较大的那个拆分样本集，它几乎就是原始的样本集，增益几乎为零；
2.影响决策树学习：决策树依赖的是数据的统计信息，而独热码编码会把数据切分到零散的小空间上。在这些零散的小空间上统计信息不准确的，学习效果变差。
本质是因为独热码编码之后的特征的表达能力较差的，特征的预测能力被人为的拆分成多份，每一份与其他特征竞争最优划分点都失败，最终该特征得到的重要性会比实际值低。
LightGBM 原生支持类别特征，采用 many-vs-many 的切分方式将类别特征分为两个子集，实现类别特征的最优切分。

2.3 与 XGBoost 的对比
本节主要总结下 LightGBM 相对于 XGBoost 的优点，从内存和速度两方面进行介绍。
2.3.1 内存更小
1.XGBoost 使用预排序后需要记录特征值及其对应样本的统计值的索引，而 LightGBM 使用了直方图算法将特征值转变为 bin 值，且不需要记录特征到样本的索引，
将空间复杂度从 [公式] 降低为 [公式] ，极大的减少了内存消耗；
2.LightGBM 采用了直方图算法将存储特征值转变为存储 bin 值，降低了内存消耗；
3.LightGBM 在训练过程中采用互斥特征捆绑算法减少了特征数量，降低了内存消耗。
2.3.2 速度更快
1.LightGBM 采用了直方图算法将遍历样本转变为遍历直方图，极大的降低了时间复杂度；
2.LightGBM 在训练过程中采用单边梯度算法过滤掉梯度小的样本，减少了大量的计算；
3.LightGBM 采用了基于 Leaf-wise 算法的增长策略构建树，减少了很多不必要的计算量；
4.LightGBM 采用优化后的特征并行、数据并行方法加速计算，当数据量非常大的时候还可以采用投票并行的策略；
5.LightGBM 对缓存也进行了优化，增加了 Cache hit 的命中率。



